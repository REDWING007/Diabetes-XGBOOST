{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd60f325-63f3-4bdd-b849-9fd1b4513bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 50, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(50, 100, 50), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(100,), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(100,), solver=adam, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(100,), solver=adam, learning_rate=\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(64, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 64), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=sgd, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=adam, learning_rate=adaptive, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=tanh, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=relu, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=constant, activation=logistic, alpha=0.1\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.0001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.05\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.001\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.01\nTuning parameters: hidden_layer_sizes=(128, 128), solver=lbfgs, learning_rate=adaptive, activation=logistic, alpha=0.1\nFitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.5s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.7s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   1.0s\n[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.8s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.8s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   1.1s\n[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   1.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   1.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.05, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   1.0s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=adam; total time=   0.0s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   1.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=constant, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=sgd; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64,), learning_rate=adaptive, solver=lbfgs; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=constant, solver=lbfgs; total time=   0.2s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=lbfgs; total time=   0.3s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=sgd; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=constant, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=adam; total time=   0.0s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(64, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=sgd; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.6s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 64), learning_rate=adaptive, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=constant, solver=lbfgs; total time=   0.9s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.4s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.8s\n[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(128, 128), learning_rate=adaptive, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=constant, solver=lbfgs; total time=   0.5s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=sgd; total time=   0.3s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.4s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=sgd; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=constant, solver=lbfgs; total time=   0.7s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=sgd; total time=   0.5s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=adam; total time=   0.1s\n[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 100, 50), learning_rate=adaptive, solver=lbfgs; total time=   0.6s"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/databricks/python/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.658008658008658"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_iris\n",
    "from time import time\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Load dataset\n",
    "#data = load_iris()\n",
    "#X = data.data\n",
    "#y = data.target\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,),(64,), (128,), (64, 64), (128, 64), (128, 128)],\n",
    "    'activation': ['tanh', 'relu', 'logistic'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.05,0.001,0.01,0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "results = []\n",
    "for hidden_layer_sizes, solver, activation, learning_rate, alpha in product(\n",
    "    param_grid['hidden_layer_sizes'],\n",
    "    param_grid['solver'],\n",
    "    param_grid['activation'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['alpha']\n",
    "):\n",
    "    print(f\"Tuning parameters: hidden_layer_sizes={hidden_layer_sizes}, solver={solver}, learning_rate={learning_rate}, activation={activation}, alpha={alpha}\")\n",
    "    start_time = time()\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "   \n",
    "    # Evaluate on training set\n",
    "    train_preds = mlp.predict(X_train)\n",
    "    train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    train_precision = precision_score(y_train, train_preds, average='macro', zero_division=0)\n",
    "    train_recall = recall_score(y_train, train_preds, average='macro')\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    valid_preds = mlp.predict(X_test)\n",
    "    valid_f1 = f1_score(y_test, valid_preds, average='macro')\n",
    "    valid_accuracy = accuracy_score(y_test, valid_preds)\n",
    "    valid_precision = precision_score(y_test, valid_preds, average='macro', zero_division=0)\n",
    "    valid_recall = recall_score(y_test, valid_preds, average='macro')\n",
    "\n",
    "    results.append({\n",
    "        'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'solver': solver,\n",
    "        'activation': activation,\n",
    "        'train_f1': train_f1,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_precision': train_precision,\n",
    "        'train_recall': train_recall,\n",
    "        'valid_f1': valid_f1,\n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'valid_precision': valid_precision,\n",
    "        'valid_recall': valid_recall,\n",
    "        'training_time': end_time - start_time\n",
    "    })\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(max_iter=100, early_stopping=True, validation_fraction=0.1, random_state=42),\n",
    "    param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_mlp = grid_search.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "y_predictions = best_mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a997116e-6b42-4aec-9b2d-ad76ee411281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6</td>\n",
       "      <td>144</td>\n",
       "      <td>72</td>\n",
       "      <td>27</td>\n",
       "      <td>228</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.255</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>62</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>0.130</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>76</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.323</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>93</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>28.7</td>\n",
       "      <td>0.356</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>90</td>\n",
       "      <td>51</td>\n",
       "      <td>220</td>\n",
       "      <td>49.7</td>\n",
       "      <td>0.325</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
       "0             6      148             72  ...                     0.627   50        1\n",
       "1             1       85             66  ...                     0.351   31        0\n",
       "2             8      183             64  ...                     0.672   32        1\n",
       "3             1       89             66  ...                     0.167   21        0\n",
       "4             0      137             40  ...                     2.288   33        1\n",
       "..          ...      ...            ...  ...                       ...  ...      ...\n",
       "95            6      144             72  ...                     0.255   40        0\n",
       "96            2       92             62  ...                     0.130   24        0\n",
       "97            1       71             48  ...                     0.323   22        0\n",
       "98            6       93             50  ...                     0.356   23        0\n",
       "99            1      122             90  ...                     0.325   31        1\n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f687131-dfac-4338-a081-8cf4f7b8c1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[107,  44],\n",
       "       [ 35,  45]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2b40c2-c70b-475d-94c9-3e44757ef3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hidden_layer_sizes</th><th>solver</th><th>activation</th><th>train_f1</th><th>train_accuracy</th><th>train_precision</th><th>train_recall</th><th>valid_f1</th><th>valid_accuracy</th><th>valid_precision</th><th>valid_recall</th><th>training_time</th></tr></thead><tbody><tr><td>List(100)</td><td>lbfgs</td><td>relu</td><td>0.7362610161367004</td><td>0.7672253258845437</td><td>0.7459921798631476</td><td>0.7301255867829055</td><td>0.6857142857142857</td><td>0.7272727272727273</td><td>0.6975791772843102</td><td>0.6797185430463577</td><td>0.3181283473968506</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>relu</td><td>0.7362610161367004</td><td>0.7672253258845437</td><td>0.7459921798631476</td><td>0.7301255867829055</td><td>0.6857142857142857</td><td>0.7272727272727273</td><td>0.6975791772843102</td><td>0.6797185430463577</td><td>0.2503702640533447</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>relu</td><td>0.7362610161367004</td><td>0.7672253258845437</td><td>0.7459921798631476</td><td>0.7301255867829055</td><td>0.6857142857142857</td><td>0.7272727272727273</td><td>0.6975791772843102</td><td>0.6797185430463577</td><td>0.3251938819885254</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>relu</td><td>0.7362610161367004</td><td>0.7672253258845437</td><td>0.7459921798631476</td><td>0.7301255867829055</td><td>0.6857142857142857</td><td>0.7272727272727273</td><td>0.6975791772843102</td><td>0.6797185430463577</td><td>0.17380976676940918</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>relu</td><td>0.7447006636203259</td><td>0.7746741154562383</td><td>0.7547409579667643</td><td>0.7383100652319697</td><td>0.6595283509623722</td><td>0.7056277056277056</td><td>0.6712121212121211</td><td>0.6543460264900662</td><td>0.49202775955200195</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>relu</td><td>0.7447006636203259</td><td>0.7746741154562383</td><td>0.7547409579667643</td><td>0.7383100652319697</td><td>0.6595283509623722</td><td>0.7056277056277056</td><td>0.6712121212121211</td><td>0.6543460264900662</td><td>0.46242570877075195</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>relu</td><td>0.7447006636203259</td><td>0.7746741154562383</td><td>0.7547409579667643</td><td>0.7383100652319697</td><td>0.6595283509623722</td><td>0.7056277056277056</td><td>0.6712121212121211</td><td>0.6543460264900662</td><td>0.417621374130249</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>relu</td><td>0.7447006636203259</td><td>0.7746741154562383</td><td>0.7547409579667643</td><td>0.7383100652319697</td><td>0.6595283509623722</td><td>0.7056277056277056</td><td>0.6712121212121211</td><td>0.6543460264900662</td><td>0.548201322555542</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>logistic</td><td>0.7484358127421712</td><td>0.7802607076350093</td><td>0.7631341638555826</td><td>0.7401542400780345</td><td>0.6543133802816903</td><td>0.7056277056277056</td><td>0.671168161862951</td><td>0.6484685430463577</td><td>1.2666685581207275</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>logistic</td><td>0.7484358127421712</td><td>0.7802607076350093</td><td>0.7631341638555826</td><td>0.7401542400780345</td><td>0.6543133802816903</td><td>0.7056277056277056</td><td>0.671168161862951</td><td>0.6484685430463577</td><td>1.4172544479370117</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>logistic</td><td>0.7484358127421712</td><td>0.7802607076350093</td><td>0.7631341638555826</td><td>0.7401542400780345</td><td>0.6543133802816903</td><td>0.7056277056277056</td><td>0.671168161862951</td><td>0.6484685430463577</td><td>1.263594388961792</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>logistic</td><td>0.7484358127421712</td><td>0.7802607076350093</td><td>0.7631341638555826</td><td>0.7401542400780345</td><td>0.6543133802816903</td><td>0.7056277056277056</td><td>0.671168161862951</td><td>0.6484685430463577</td><td>1.1605496406555176</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>relu</td><td>0.6906012264535235</td><td>0.707635009310987</td><td>0.6876196052669161</td><td>0.7002301408278973</td><td>0.6527140370311757</td><td>0.670995670995671</td><td>0.6511815252416756</td><td>0.6631208609271523</td><td>0.15809273719787598</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>relu</td><td>0.6906012264535235</td><td>0.707635009310987</td><td>0.6876196052669161</td><td>0.7002301408278973</td><td>0.6527140370311757</td><td>0.670995670995671</td><td>0.6511815252416756</td><td>0.6631208609271523</td><td>0.15519165992736816</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>relu</td><td>0.6906012264535235</td><td>0.707635009310987</td><td>0.6876196052669161</td><td>0.7002301408278973</td><td>0.6527140370311757</td><td>0.670995670995671</td><td>0.6511815252416756</td><td>0.6631208609271523</td><td>0.2644214630126953</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>relu</td><td>0.6906012264535235</td><td>0.707635009310987</td><td>0.6876196052669161</td><td>0.7002301408278973</td><td>0.6527140370311757</td><td>0.670995670995671</td><td>0.6511815252416756</td><td>0.6631208609271523</td><td>0.12277388572692871</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>relu</td><td>0.7394917216988514</td><td>0.7690875232774674</td><td>0.7476771196283392</td><td>0.7340120709626288</td><td>0.6520485453606473</td><td>0.696969696969697</td><td>0.6609978347167087</td><td>0.6477235099337748</td><td>1.0708999633789062</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>relu</td><td>0.7394917216988514</td><td>0.7690875232774674</td><td>0.7476771196283392</td><td>0.7340120709626288</td><td>0.6520485453606473</td><td>0.696969696969697</td><td>0.6609978347167087</td><td>0.6477235099337748</td><td>0.8253815174102783</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>relu</td><td>0.7394917216988514</td><td>0.7690875232774674</td><td>0.7476771196283392</td><td>0.7340120709626288</td><td>0.6520485453606473</td><td>0.696969696969697</td><td>0.6609978347167087</td><td>0.6477235099337748</td><td>0.8473672866821289</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>relu</td><td>0.7394917216988514</td><td>0.7690875232774674</td><td>0.7476771196283392</td><td>0.7340120709626288</td><td>0.6520485453606473</td><td>0.696969696969697</td><td>0.6609978347167087</td><td>0.6477235099337748</td><td>1.989081859588623</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>tanh</td><td>0.809345188478281</td><td>0.8286778398510242</td><td>0.8138008200056337</td><td>0.8056224471133329</td><td>0.6366721854304636</td><td>0.670995670995671</td><td>0.6366721854304636</td><td>0.6366721854304636</td><td>0.5716042518615723</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>tanh</td><td>0.809345188478281</td><td>0.8286778398510242</td><td>0.8138008200056337</td><td>0.8056224471133329</td><td>0.6366721854304636</td><td>0.670995670995671</td><td>0.6366721854304636</td><td>0.6366721854304636</td><td>0.619905948638916</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>tanh</td><td>0.809345188478281</td><td>0.8286778398510242</td><td>0.8138008200056337</td><td>0.8056224471133329</td><td>0.6366721854304636</td><td>0.670995670995671</td><td>0.6366721854304636</td><td>0.6366721854304636</td><td>0.35243988037109375</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>tanh</td><td>0.809345188478281</td><td>0.8286778398510242</td><td>0.8138008200056337</td><td>0.8056224471133329</td><td>0.6366721854304636</td><td>0.670995670995671</td><td>0.6366721854304636</td><td>0.6366721854304636</td><td>0.41698479652404785</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>tanh</td><td>0.8054385725694919</td><td>0.8268156424581006</td><td>0.813752276867031</td><td>0.7992821435103334</td><td>0.6335427374833428</td><td>0.6753246753246753</td><td>0.6373764522281948</td><td>0.6311672185430464</td><td>0.16018366813659668</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>tanh</td><td>0.8054385725694919</td><td>0.8268156424581006</td><td>0.813752276867031</td><td>0.7992821435103334</td><td>0.6335427374833428</td><td>0.6753246753246753</td><td>0.6373764522281948</td><td>0.6311672185430464</td><td>0.16567230224609375</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>tanh</td><td>0.8054385725694919</td><td>0.8268156424581006</td><td>0.813752276867031</td><td>0.7992821435103334</td><td>0.6335427374833428</td><td>0.6753246753246753</td><td>0.6373764522281948</td><td>0.6311672185430464</td><td>0.22291064262390137</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>tanh</td><td>0.8054385725694919</td><td>0.8268156424581006</td><td>0.813752276867031</td><td>0.7992821435103334</td><td>0.6335427374833428</td><td>0.6753246753246753</td><td>0.6373764522281948</td><td>0.6311672185430464</td><td>0.17378807067871094</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>logistic</td><td>0.6945379808590588</td><td>0.74487895716946</td><td>0.7278701161959915</td><td>0.684714686337865</td><td>0.6141167561306992</td><td>0.6753246753246753</td><td>0.6316515569570358</td><td>0.6105960264900663</td><td>0.7542424201965332</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>logistic</td><td>0.6945379808590588</td><td>0.74487895716946</td><td>0.7278701161959915</td><td>0.684714686337865</td><td>0.6141167561306992</td><td>0.6753246753246753</td><td>0.6316515569570358</td><td>0.6105960264900663</td><td>0.5275087356567383</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>logistic</td><td>0.6945379808590588</td><td>0.74487895716946</td><td>0.7278701161959915</td><td>0.684714686337865</td><td>0.6141167561306992</td><td>0.6753246753246753</td><td>0.6316515569570358</td><td>0.6105960264900663</td><td>0.621863603591919</td></tr><tr><td>List(50, 50, 50)</td><td>lbfgs</td><td>logistic</td><td>0.6945379808590588</td><td>0.74487895716946</td><td>0.7278701161959915</td><td>0.684714686337865</td><td>0.6141167561306992</td><td>0.6753246753246753</td><td>0.6316515569570358</td><td>0.6105960264900663</td><td>0.6106851100921631</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>relu</td><td>0.639631866551625</td><td>0.6871508379888268</td><td>0.6506137465667492</td><td>0.6353944400414558</td><td>0.6075433231396534</td><td>0.6753246753246753</td><td>0.6306818181818181</td><td>0.6047185430463576</td><td>0.2808108329772949</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>relu</td><td>0.639631866551625</td><td>0.6871508379888268</td><td>0.6506137465667492</td><td>0.6353944400414558</td><td>0.6075433231396534</td><td>0.6753246753246753</td><td>0.6306818181818181</td><td>0.6047185430463576</td><td>0.45590925216674805</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>relu</td><td>0.639631866551625</td><td>0.6871508379888268</td><td>0.6506137465667492</td><td>0.6353944400414558</td><td>0.6075433231396534</td><td>0.6753246753246753</td><td>0.6306818181818181</td><td>0.6047185430463576</td><td>0.1856365203857422</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>relu</td><td>0.639631866551625</td><td>0.6871508379888268</td><td>0.6506137465667492</td><td>0.6353944400414558</td><td>0.6075433231396534</td><td>0.6753246753246753</td><td>0.6306818181818181</td><td>0.6047185430463576</td><td>0.33611202239990234</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>logistic</td><td>0.8185810810810812</td><td>0.8361266294227188</td><td>0.8211654542640459</td><td>0.8162607449856734</td><td>0.569403502687706</td><td>0.6277056277056277</td><td>0.5757575757575758</td><td>0.568294701986755</td><td>0.30056285858154297</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>logistic</td><td>0.8185810810810812</td><td>0.8361266294227188</td><td>0.8211654542640459</td><td>0.8162607449856734</td><td>0.569403502687706</td><td>0.6277056277056277</td><td>0.5757575757575758</td><td>0.568294701986755</td><td>0.21281147003173828</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>logistic</td><td>0.8185810810810812</td><td>0.8361266294227188</td><td>0.8211654542640459</td><td>0.8162607449856734</td><td>0.569403502687706</td><td>0.6277056277056277</td><td>0.5757575757575758</td><td>0.568294701986755</td><td>0.23068022727966309</td></tr><tr><td>List(100)</td><td>lbfgs</td><td>logistic</td><td>0.8185810810810812</td><td>0.8361266294227188</td><td>0.8211654542640459</td><td>0.8162607449856734</td><td>0.569403502687706</td><td>0.6277056277056277</td><td>0.5757575757575758</td><td>0.568294701986755</td><td>0.33745288848876953</td></tr><tr><td>List(100)</td><td>sgd</td><td>relu</td><td>0.5510715914272686</td><td>0.6722532588454376</td><td>0.635469562557357</td><td>0.5674952752545266</td><td>0.5675009854158455</td><td>0.670995670995671</td><td>0.6234660485410417</td><td>0.5749586092715232</td><td>0.05044269561767578</td></tr><tr><td>List(100)</td><td>sgd</td><td>relu</td><td>0.5510715914272686</td><td>0.6722532588454376</td><td>0.635469562557357</td><td>0.5674952752545266</td><td>0.5675009854158455</td><td>0.670995670995671</td><td>0.6234660485410417</td><td>0.5749586092715232</td><td>0.054100751876831055</td></tr><tr><td>List(100)</td><td>sgd</td><td>relu</td><td>0.5510715914272686</td><td>0.6722532588454376</td><td>0.635469562557357</td><td>0.5674952752545266</td><td>0.5675009854158455</td><td>0.670995670995671</td><td>0.6234660485410417</td><td>0.5749586092715232</td><td>0.05282306671142578</td></tr><tr><td>List(100)</td><td>sgd</td><td>relu</td><td>0.5510715914272686</td><td>0.6722532588454376</td><td>0.635469562557357</td><td>0.5674952752545266</td><td>0.5675009854158455</td><td>0.670995670995671</td><td>0.6234660485410417</td><td>0.5749586092715232</td><td>0.05336356163024902</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>tanh</td><td>0.5598360655737705</td><td>0.6908752327746741</td><td>0.6933286308286308</td><td>0.5793681033957203</td><td>0.5617381812511371</td><td>0.683982683982684</td><td>0.6568794810515535</td><td>0.5760761589403973</td><td>0.11682438850402832</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>tanh</td><td>0.5598360655737705</td><td>0.6908752327746741</td><td>0.6933286308286308</td><td>0.5793681033957203</td><td>0.5617381812511371</td><td>0.683982683982684</td><td>0.6568794810515535</td><td>0.5760761589403973</td><td>0.17468619346618652</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>tanh</td><td>0.5598360655737705</td><td>0.6908752327746741</td><td>0.6933286308286308</td><td>0.5793681033957203</td><td>0.5617381812511371</td><td>0.683982683982684</td><td>0.6568794810515535</td><td>0.5760761589403973</td><td>0.11877083778381348</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>tanh</td><td>0.5598360655737705</td><td>0.6908752327746741</td><td>0.6933286308286308</td><td>0.5793681033957203</td><td>0.5617381812511371</td><td>0.683982683982684</td><td>0.6568794810515535</td><td>0.5760761589403973</td><td>0.16115093231201172</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>tanh</td><td>0.778472116119175</td><td>0.7988826815642458</td><td>0.7791793033728518</td><td>0.7777921721636286</td><td>0.5523732458848832</td><td>0.6060606060606061</td><td>0.5550176056338028</td><td>0.5517384105960265</td><td>0.6389546394348145</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>tanh</td><td>0.778472116119175</td><td>0.7988826815642458</td><td>0.7791793033728518</td><td>0.7777921721636286</td><td>0.5523732458848832</td><td>0.6060606060606061</td><td>0.5550176056338028</td><td>0.5517384105960265</td><td>0.5658309459686279</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>tanh</td><td>0.778472116119175</td><td>0.7988826815642458</td><td>0.7791793033728518</td><td>0.7777921721636286</td><td>0.5523732458848832</td><td>0.6060606060606061</td><td>0.5550176056338028</td><td>0.5517384105960265</td><td>0.8675565719604492</td></tr><tr><td>List(50, 100, 50)</td><td>lbfgs</td><td>tanh</td><td>0.778472116119175</td><td>0.7988826815642458</td><td>0.7791793033728518</td><td>0.7777921721636286</td><td>0.5523732458848832</td><td>0.6060606060606061</td><td>0.5550176056338028</td><td>0.5517384105960265</td><td>0.8620691299438477</td></tr><tr><td>List(100)</td><td>adam</td><td>relu</td><td>0.6321077871660197</td><td>0.6927374301675978</td><td>0.6571936223818298</td><td>0.6274233371944157</td><td>0.551271796512558</td><td>0.6320346320346321</td><td>0.5691117235531058</td><td>0.5539735099337748</td><td>0.11571836471557617</td></tr><tr><td>List(100)</td><td>adam</td><td>relu</td><td>0.6321077871660197</td><td>0.6927374301675978</td><td>0.6571936223818298</td><td>0.6274233371944157</td><td>0.551271796512558</td><td>0.6320346320346321</td><td>0.5691117235531058</td><td>0.5539735099337748</td><td>0.10072779655456543</td></tr><tr><td>List(100)</td><td>adam</td><td>relu</td><td>0.6321077871660197</td><td>0.6927374301675978</td><td>0.6571936223818298</td><td>0.6274233371944157</td><td>0.551271796512558</td><td>0.6320346320346321</td><td>0.5691117235531058</td><td>0.5539735099337748</td><td>0.08798336982727051</td></tr><tr><td>List(100)</td><td>adam</td><td>relu</td><td>0.6321077871660197</td><td>0.6927374301675978</td><td>0.6571936223818298</td><td>0.6274233371944157</td><td>0.551271796512558</td><td>0.6320346320346321</td><td>0.5691117235531058</td><td>0.5539735099337748</td><td>0.06615424156188965</td></tr><tr><td>List(100)</td><td>sgd</td><td>tanh</td><td>0.6262879143921045</td><td>0.7132216014897579</td><td>0.7035048471290082</td><td>0.6247790038407608</td><td>0.5377238350183227</td><td>0.6666666666666666</td><td>0.6174462273813588</td><td>0.5569536423841059</td><td>0.0701742172241211</td></tr><tr><td>List(100)</td><td>sgd</td><td>tanh</td><td>0.6262879143921045</td><td>0.7132216014897579</td><td>0.7035048471290082</td><td>0.6247790038407608</td><td>0.5377238350183227</td><td>0.6666666666666666</td><td>0.6174462273813588</td><td>0.5569536423841059</td><td>0.1341092586517334</td></tr><tr><td>List(100)</td><td>sgd</td><td>tanh</td><td>0.6262879143921045</td><td>0.7132216014897579</td><td>0.7035048471290082</td><td>0.6247790038407608</td><td>0.5377238350183227</td><td>0.6666666666666666</td><td>0.6174462273813588</td><td>0.5569536423841059</td><td>0.14577150344848633</td></tr><tr><td>List(100)</td><td>sgd</td><td>tanh</td><td>0.6262879143921045</td><td>0.7132216014897579</td><td>0.7035048471290082</td><td>0.6247790038407608</td><td>0.5377238350183227</td><td>0.6666666666666666</td><td>0.6174462273813588</td><td>0.5569536423841059</td><td>0.0716860294342041</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>relu</td><td>0.5518145398845292</td><td>0.6834264432029795</td><td>0.6710100064790152</td><td>0.5724105346582942</td><td>0.5375058281451415</td><td>0.683982683982684</td><td>0.6761904761904762</td><td>0.5643211920529801</td><td>0.10633683204650879</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>relu</td><td>0.5518145398845292</td><td>0.6834264432029795</td><td>0.6710100064790152</td><td>0.5724105346582942</td><td>0.5375058281451415</td><td>0.683982683982684</td><td>0.6761904761904762</td><td>0.5643211920529801</td><td>0.1101837158203125</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>relu</td><td>0.5518145398845292</td><td>0.6834264432029795</td><td>0.6710100064790152</td><td>0.5724105346582942</td><td>0.5375058281451415</td><td>0.683982683982684</td><td>0.6761904761904762</td><td>0.5643211920529801</td><td>0.13660240173339844</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>relu</td><td>0.5518145398845292</td><td>0.6834264432029795</td><td>0.6710100064790152</td><td>0.5724105346582942</td><td>0.5375058281451415</td><td>0.683982683982684</td><td>0.6761904761904762</td><td>0.5643211920529801</td><td>0.22317242622375488</td></tr><tr><td>List(100)</td><td>adam</td><td>tanh</td><td>0.5908015975896017</td><td>0.6759776536312849</td><td>0.6346931071238231</td><td>0.5924449795769067</td><td>0.5021181716833891</td><td>0.6233766233766234</td><td>0.5351769295068264</td><td>0.5209023178807948</td><td>0.04518389701843262</td></tr><tr><td>List(100)</td><td>adam</td><td>tanh</td><td>0.5908015975896017</td><td>0.6759776536312849</td><td>0.6346931071238231</td><td>0.5924449795769067</td><td>0.5021181716833891</td><td>0.6233766233766234</td><td>0.5351769295068264</td><td>0.5209023178807948</td><td>0.07428812980651855</td></tr><tr><td>List(100)</td><td>adam</td><td>tanh</td><td>0.5908015975896017</td><td>0.6759776536312849</td><td>0.6346931071238231</td><td>0.5924449795769067</td><td>0.5021181716833891</td><td>0.6233766233766234</td><td>0.5351769295068264</td><td>0.5209023178807948</td><td>0.051788330078125</td></tr><tr><td>List(100)</td><td>adam</td><td>tanh</td><td>0.5908015975896017</td><td>0.6759776536312849</td><td>0.6346931071238231</td><td>0.5924449795769067</td><td>0.5021181716833891</td><td>0.6233766233766234</td><td>0.5351769295068264</td><td>0.5209023178807948</td><td>0.055976152420043945</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>logistic</td><td>0.5014364292471009</td><td>0.6052141527001862</td><td>0.5214069994670457</td><td>0.5146924343107968</td><td>0.49921166732361055</td><td>0.6190476190476191</td><td>0.5289746386692119</td><td>0.517591059602649</td><td>0.1811237335205078</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>logistic</td><td>0.5014364292471009</td><td>0.6052141527001862</td><td>0.5214069994670457</td><td>0.5146924343107968</td><td>0.49921166732361055</td><td>0.6190476190476191</td><td>0.5289746386692119</td><td>0.517591059602649</td><td>0.13753128051757812</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>logistic</td><td>0.5014364292471009</td><td>0.6052141527001862</td><td>0.5214069994670457</td><td>0.5146924343107968</td><td>0.49921166732361055</td><td>0.6190476190476191</td><td>0.5289746386692119</td><td>0.517591059602649</td><td>0.2401723861694336</td></tr><tr><td>List(50, 100, 50)</td><td>adam</td><td>logistic</td><td>0.5014364292471009</td><td>0.6052141527001862</td><td>0.5214069994670457</td><td>0.5146924343107968</td><td>0.49921166732361055</td><td>0.6190476190476191</td><td>0.5289746386692119</td><td>0.517591059602649</td><td>0.15000700950622559</td></tr><tr><td>List(100)</td><td>adam</td><td>logistic</td><td>0.4957137286990714</td><td>0.6815642458100558</td><td>0.7713584841820336</td><td>0.5488934950923612</td><td>0.49514817728822447</td><td>0.6753246753246753</td><td>0.6833098094565984</td><td>0.5430049668874172</td><td>0.1176602840423584</td></tr><tr><td>List(100)</td><td>adam</td><td>logistic</td><td>0.4957137286990714</td><td>0.6815642458100558</td><td>0.7713584841820336</td><td>0.5488934950923612</td><td>0.49514817728822447</td><td>0.6753246753246753</td><td>0.6833098094565984</td><td>0.5430049668874172</td><td>0.10259413719177246</td></tr><tr><td>List(100)</td><td>adam</td><td>logistic</td><td>0.4957137286990714</td><td>0.6815642458100558</td><td>0.7713584841820336</td><td>0.5488934950923612</td><td>0.49514817728822447</td><td>0.6753246753246753</td><td>0.6833098094565984</td><td>0.5430049668874172</td><td>0.11374688148498535</td></tr><tr><td>List(100)</td><td>adam</td><td>logistic</td><td>0.4957137286990714</td><td>0.6815642458100558</td><td>0.7713584841820336</td><td>0.5488934950923612</td><td>0.49514817728822447</td><td>0.6753246753246753</td><td>0.6833098094565984</td><td>0.5430049668874172</td><td>0.08202672004699707</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>tanh</td><td>0.4651873554193694</td><td>0.6480446927374302</td><td>0.5725384177575412</td><td>0.5194248003414009</td><td>0.4765810985228739</td><td>0.658008658008658</td><td>0.6</td><td>0.5268211920529802</td><td>0.14359140396118164</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>tanh</td><td>0.4651873554193694</td><td>0.6480446927374302</td><td>0.5725384177575412</td><td>0.5194248003414009</td><td>0.4765810985228739</td><td>0.658008658008658</td><td>0.6</td><td>0.5268211920529802</td><td>0.06280994415283203</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>tanh</td><td>0.4651873554193694</td><td>0.6480446927374302</td><td>0.5725384177575412</td><td>0.5194248003414009</td><td>0.4765810985228739</td><td>0.658008658008658</td><td>0.6</td><td>0.5268211920529802</td><td>0.05563211441040039</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>tanh</td><td>0.4651873554193694</td><td>0.6480446927374302</td><td>0.5725384177575412</td><td>0.5194248003414009</td><td>0.4765810985228739</td><td>0.658008658008658</td><td>0.6</td><td>0.5268211920529802</td><td>0.06773233413696289</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>relu</td><td>0.42954179798558617</td><td>0.5512104283054003</td><td>0.430738486692706</td><td>0.4535146009876242</td><td>0.4448736664795059</td><td>0.5367965367965368</td><td>0.4447973381730187</td><td>0.4546771523178808</td><td>0.07741045951843262</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>relu</td><td>0.42954179798558617</td><td>0.5512104283054003</td><td>0.430738486692706</td><td>0.4535146009876242</td><td>0.4448736664795059</td><td>0.5367965367965368</td><td>0.4447973381730187</td><td>0.4546771523178808</td><td>0.07506155967712402</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>relu</td><td>0.42954179798558617</td><td>0.5512104283054003</td><td>0.430738486692706</td><td>0.4535146009876242</td><td>0.4448736664795059</td><td>0.5367965367965368</td><td>0.4447973381730187</td><td>0.4546771523178808</td><td>0.0732572078704834</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>relu</td><td>0.42954179798558617</td><td>0.5512104283054003</td><td>0.430738486692706</td><td>0.4535146009876242</td><td>0.4448736664795059</td><td>0.5367965367965368</td><td>0.4447973381730187</td><td>0.4546771523178808</td><td>0.07546567916870117</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>tanh</td><td>0.4225130802534142</td><td>0.6405959031657356</td><td>0.5095001016053647</td><td>0.5014250441992318</td><td>0.3981609195402299</td><td>0.6320346320346321</td><td>0.3950892857142857</td><td>0.4863824503311258</td><td>0.22121381759643555</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>tanh</td><td>0.4225130802534142</td><td>0.6405959031657356</td><td>0.5095001016053647</td><td>0.5014250441992318</td><td>0.3981609195402299</td><td>0.6320346320346321</td><td>0.3950892857142857</td><td>0.4863824503311258</td><td>0.26922035217285156</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>tanh</td><td>0.4225130802534142</td><td>0.6405959031657356</td><td>0.5095001016053647</td><td>0.5014250441992318</td><td>0.3981609195402299</td><td>0.6320346320346321</td><td>0.3950892857142857</td><td>0.4863824503311258</td><td>0.22985124588012695</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>tanh</td><td>0.4225130802534142</td><td>0.6405959031657356</td><td>0.5095001016053647</td><td>0.5014250441992318</td><td>0.3981609195402299</td><td>0.6320346320346321</td><td>0.3950892857142857</td><td>0.4863824503311258</td><td>0.08966827392578125</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.14042186737060547</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.12438416481018066</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.159318208694458</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.20062494277954102</td></tr><tr><td>List(50, 50, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.1353003978729248</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.1016993522644043</td></tr><tr><td>List(100)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.07140898704528809</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.1459331512451172</td></tr><tr><td>List(100)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.04213213920593262</td></tr><tr><td>List(100)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.04602384567260742</td></tr><tr><td>List(100)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.08082866668701172</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.3765523433685303</td></tr><tr><td>List(50, 100, 50)</td><td>sgd</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.21715283393859863</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.08377265930175781</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.07906794548034668</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>logistic</td><td>0.39390519187358913</td><td>0.6499068901303539</td><td>0.3249534450651769</td><td>0.5</td><td>0.39528795811518325</td><td>0.6536796536796536</td><td>0.3268398268398268</td><td>0.5</td><td>0.09014225006103516</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>tanh</td><td>0.396781801179771</td><td>0.6443202979515829</td><td>0.4242481203007519</td><td>0.49692891544229717</td><td>0.39370078740157477</td><td>0.6493506493506493</td><td>0.32608695652173914</td><td>0.4966887417218543</td><td>0.06585979461669922</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>tanh</td><td>0.396781801179771</td><td>0.6443202979515829</td><td>0.4242481203007519</td><td>0.49692891544229717</td><td>0.39370078740157477</td><td>0.6493506493506493</td><td>0.32608695652173914</td><td>0.4966887417218543</td><td>0.08847403526306152</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>tanh</td><td>0.396781801179771</td><td>0.6443202979515829</td><td>0.4242481203007519</td><td>0.49692891544229717</td><td>0.39370078740157477</td><td>0.6493506493506493</td><td>0.32608695652173914</td><td>0.4966887417218543</td><td>0.10080909729003906</td></tr><tr><td>List(50, 50, 50)</td><td>adam</td><td>tanh</td><td>0.396781801179771</td><td>0.6443202979515829</td><td>0.4242481203007519</td><td>0.49692891544229717</td><td>0.39370078740157477</td><td>0.6493506493506493</td><td>0.32608695652173914</td><td>0.4966887417218543</td><td>0.06584525108337402</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          100
         ],
         "lbfgs",
         "relu",
         0.7362610161367004,
         0.7672253258845437,
         0.7459921798631476,
         0.7301255867829055,
         0.6857142857142857,
         0.7272727272727273,
         0.6975791772843102,
         0.6797185430463577,
         0.3181283473968506
        ],
        [
         [
          100
         ],
         "lbfgs",
         "relu",
         0.7362610161367004,
         0.7672253258845437,
         0.7459921798631476,
         0.7301255867829055,
         0.6857142857142857,
         0.7272727272727273,
         0.6975791772843102,
         0.6797185430463577,
         0.2503702640533447
        ],
        [
         [
          100
         ],
         "lbfgs",
         "relu",
         0.7362610161367004,
         0.7672253258845437,
         0.7459921798631476,
         0.7301255867829055,
         0.6857142857142857,
         0.7272727272727273,
         0.6975791772843102,
         0.6797185430463577,
         0.3251938819885254
        ],
        [
         [
          100
         ],
         "lbfgs",
         "relu",
         0.7362610161367004,
         0.7672253258845437,
         0.7459921798631476,
         0.7301255867829055,
         0.6857142857142857,
         0.7272727272727273,
         0.6975791772843102,
         0.6797185430463577,
         0.17380976676940918
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "relu",
         0.7447006636203259,
         0.7746741154562383,
         0.7547409579667643,
         0.7383100652319697,
         0.6595283509623722,
         0.7056277056277056,
         0.6712121212121211,
         0.6543460264900662,
         0.49202775955200195
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "relu",
         0.7447006636203259,
         0.7746741154562383,
         0.7547409579667643,
         0.7383100652319697,
         0.6595283509623722,
         0.7056277056277056,
         0.6712121212121211,
         0.6543460264900662,
         0.46242570877075195
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "relu",
         0.7447006636203259,
         0.7746741154562383,
         0.7547409579667643,
         0.7383100652319697,
         0.6595283509623722,
         0.7056277056277056,
         0.6712121212121211,
         0.6543460264900662,
         0.417621374130249
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "relu",
         0.7447006636203259,
         0.7746741154562383,
         0.7547409579667643,
         0.7383100652319697,
         0.6595283509623722,
         0.7056277056277056,
         0.6712121212121211,
         0.6543460264900662,
         0.548201322555542
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "logistic",
         0.7484358127421712,
         0.7802607076350093,
         0.7631341638555826,
         0.7401542400780345,
         0.6543133802816903,
         0.7056277056277056,
         0.671168161862951,
         0.6484685430463577,
         1.2666685581207275
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "logistic",
         0.7484358127421712,
         0.7802607076350093,
         0.7631341638555826,
         0.7401542400780345,
         0.6543133802816903,
         0.7056277056277056,
         0.671168161862951,
         0.6484685430463577,
         1.4172544479370117
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "logistic",
         0.7484358127421712,
         0.7802607076350093,
         0.7631341638555826,
         0.7401542400780345,
         0.6543133802816903,
         0.7056277056277056,
         0.671168161862951,
         0.6484685430463577,
         1.263594388961792
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "logistic",
         0.7484358127421712,
         0.7802607076350093,
         0.7631341638555826,
         0.7401542400780345,
         0.6543133802816903,
         0.7056277056277056,
         0.671168161862951,
         0.6484685430463577,
         1.1605496406555176
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "relu",
         0.6906012264535235,
         0.707635009310987,
         0.6876196052669161,
         0.7002301408278973,
         0.6527140370311757,
         0.670995670995671,
         0.6511815252416756,
         0.6631208609271523,
         0.15809273719787598
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "relu",
         0.6906012264535235,
         0.707635009310987,
         0.6876196052669161,
         0.7002301408278973,
         0.6527140370311757,
         0.670995670995671,
         0.6511815252416756,
         0.6631208609271523,
         0.15519165992736816
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "relu",
         0.6906012264535235,
         0.707635009310987,
         0.6876196052669161,
         0.7002301408278973,
         0.6527140370311757,
         0.670995670995671,
         0.6511815252416756,
         0.6631208609271523,
         0.2644214630126953
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "relu",
         0.6906012264535235,
         0.707635009310987,
         0.6876196052669161,
         0.7002301408278973,
         0.6527140370311757,
         0.670995670995671,
         0.6511815252416756,
         0.6631208609271523,
         0.12277388572692871
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "relu",
         0.7394917216988514,
         0.7690875232774674,
         0.7476771196283392,
         0.7340120709626288,
         0.6520485453606473,
         0.696969696969697,
         0.6609978347167087,
         0.6477235099337748,
         1.0708999633789062
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "relu",
         0.7394917216988514,
         0.7690875232774674,
         0.7476771196283392,
         0.7340120709626288,
         0.6520485453606473,
         0.696969696969697,
         0.6609978347167087,
         0.6477235099337748,
         0.8253815174102783
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "relu",
         0.7394917216988514,
         0.7690875232774674,
         0.7476771196283392,
         0.7340120709626288,
         0.6520485453606473,
         0.696969696969697,
         0.6609978347167087,
         0.6477235099337748,
         0.8473672866821289
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "relu",
         0.7394917216988514,
         0.7690875232774674,
         0.7476771196283392,
         0.7340120709626288,
         0.6520485453606473,
         0.696969696969697,
         0.6609978347167087,
         0.6477235099337748,
         1.989081859588623
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "tanh",
         0.809345188478281,
         0.8286778398510242,
         0.8138008200056337,
         0.8056224471133329,
         0.6366721854304636,
         0.670995670995671,
         0.6366721854304636,
         0.6366721854304636,
         0.5716042518615723
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "tanh",
         0.809345188478281,
         0.8286778398510242,
         0.8138008200056337,
         0.8056224471133329,
         0.6366721854304636,
         0.670995670995671,
         0.6366721854304636,
         0.6366721854304636,
         0.619905948638916
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "tanh",
         0.809345188478281,
         0.8286778398510242,
         0.8138008200056337,
         0.8056224471133329,
         0.6366721854304636,
         0.670995670995671,
         0.6366721854304636,
         0.6366721854304636,
         0.35243988037109375
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "tanh",
         0.809345188478281,
         0.8286778398510242,
         0.8138008200056337,
         0.8056224471133329,
         0.6366721854304636,
         0.670995670995671,
         0.6366721854304636,
         0.6366721854304636,
         0.41698479652404785
        ],
        [
         [
          100
         ],
         "lbfgs",
         "tanh",
         0.8054385725694919,
         0.8268156424581006,
         0.813752276867031,
         0.7992821435103334,
         0.6335427374833428,
         0.6753246753246753,
         0.6373764522281948,
         0.6311672185430464,
         0.16018366813659668
        ],
        [
         [
          100
         ],
         "lbfgs",
         "tanh",
         0.8054385725694919,
         0.8268156424581006,
         0.813752276867031,
         0.7992821435103334,
         0.6335427374833428,
         0.6753246753246753,
         0.6373764522281948,
         0.6311672185430464,
         0.16567230224609375
        ],
        [
         [
          100
         ],
         "lbfgs",
         "tanh",
         0.8054385725694919,
         0.8268156424581006,
         0.813752276867031,
         0.7992821435103334,
         0.6335427374833428,
         0.6753246753246753,
         0.6373764522281948,
         0.6311672185430464,
         0.22291064262390137
        ],
        [
         [
          100
         ],
         "lbfgs",
         "tanh",
         0.8054385725694919,
         0.8268156424581006,
         0.813752276867031,
         0.7992821435103334,
         0.6335427374833428,
         0.6753246753246753,
         0.6373764522281948,
         0.6311672185430464,
         0.17378807067871094
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "logistic",
         0.6945379808590588,
         0.74487895716946,
         0.7278701161959915,
         0.684714686337865,
         0.6141167561306992,
         0.6753246753246753,
         0.6316515569570358,
         0.6105960264900663,
         0.7542424201965332
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "logistic",
         0.6945379808590588,
         0.74487895716946,
         0.7278701161959915,
         0.684714686337865,
         0.6141167561306992,
         0.6753246753246753,
         0.6316515569570358,
         0.6105960264900663,
         0.5275087356567383
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "logistic",
         0.6945379808590588,
         0.74487895716946,
         0.7278701161959915,
         0.684714686337865,
         0.6141167561306992,
         0.6753246753246753,
         0.6316515569570358,
         0.6105960264900663,
         0.621863603591919
        ],
        [
         [
          50,
          50,
          50
         ],
         "lbfgs",
         "logistic",
         0.6945379808590588,
         0.74487895716946,
         0.7278701161959915,
         0.684714686337865,
         0.6141167561306992,
         0.6753246753246753,
         0.6316515569570358,
         0.6105960264900663,
         0.6106851100921631
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "relu",
         0.639631866551625,
         0.6871508379888268,
         0.6506137465667492,
         0.6353944400414558,
         0.6075433231396534,
         0.6753246753246753,
         0.6306818181818181,
         0.6047185430463576,
         0.2808108329772949
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "relu",
         0.639631866551625,
         0.6871508379888268,
         0.6506137465667492,
         0.6353944400414558,
         0.6075433231396534,
         0.6753246753246753,
         0.6306818181818181,
         0.6047185430463576,
         0.45590925216674805
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "relu",
         0.639631866551625,
         0.6871508379888268,
         0.6506137465667492,
         0.6353944400414558,
         0.6075433231396534,
         0.6753246753246753,
         0.6306818181818181,
         0.6047185430463576,
         0.1856365203857422
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "relu",
         0.639631866551625,
         0.6871508379888268,
         0.6506137465667492,
         0.6353944400414558,
         0.6075433231396534,
         0.6753246753246753,
         0.6306818181818181,
         0.6047185430463576,
         0.33611202239990234
        ],
        [
         [
          100
         ],
         "lbfgs",
         "logistic",
         0.8185810810810812,
         0.8361266294227188,
         0.8211654542640459,
         0.8162607449856734,
         0.569403502687706,
         0.6277056277056277,
         0.5757575757575758,
         0.568294701986755,
         0.30056285858154297
        ],
        [
         [
          100
         ],
         "lbfgs",
         "logistic",
         0.8185810810810812,
         0.8361266294227188,
         0.8211654542640459,
         0.8162607449856734,
         0.569403502687706,
         0.6277056277056277,
         0.5757575757575758,
         0.568294701986755,
         0.21281147003173828
        ],
        [
         [
          100
         ],
         "lbfgs",
         "logistic",
         0.8185810810810812,
         0.8361266294227188,
         0.8211654542640459,
         0.8162607449856734,
         0.569403502687706,
         0.6277056277056277,
         0.5757575757575758,
         0.568294701986755,
         0.23068022727966309
        ],
        [
         [
          100
         ],
         "lbfgs",
         "logistic",
         0.8185810810810812,
         0.8361266294227188,
         0.8211654542640459,
         0.8162607449856734,
         0.569403502687706,
         0.6277056277056277,
         0.5757575757575758,
         0.568294701986755,
         0.33745288848876953
        ],
        [
         [
          100
         ],
         "sgd",
         "relu",
         0.5510715914272686,
         0.6722532588454376,
         0.635469562557357,
         0.5674952752545266,
         0.5675009854158455,
         0.670995670995671,
         0.6234660485410417,
         0.5749586092715232,
         0.05044269561767578
        ],
        [
         [
          100
         ],
         "sgd",
         "relu",
         0.5510715914272686,
         0.6722532588454376,
         0.635469562557357,
         0.5674952752545266,
         0.5675009854158455,
         0.670995670995671,
         0.6234660485410417,
         0.5749586092715232,
         0.054100751876831055
        ],
        [
         [
          100
         ],
         "sgd",
         "relu",
         0.5510715914272686,
         0.6722532588454376,
         0.635469562557357,
         0.5674952752545266,
         0.5675009854158455,
         0.670995670995671,
         0.6234660485410417,
         0.5749586092715232,
         0.05282306671142578
        ],
        [
         [
          100
         ],
         "sgd",
         "relu",
         0.5510715914272686,
         0.6722532588454376,
         0.635469562557357,
         0.5674952752545266,
         0.5675009854158455,
         0.670995670995671,
         0.6234660485410417,
         0.5749586092715232,
         0.05336356163024902
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "tanh",
         0.5598360655737705,
         0.6908752327746741,
         0.6933286308286308,
         0.5793681033957203,
         0.5617381812511371,
         0.683982683982684,
         0.6568794810515535,
         0.5760761589403973,
         0.11682438850402832
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "tanh",
         0.5598360655737705,
         0.6908752327746741,
         0.6933286308286308,
         0.5793681033957203,
         0.5617381812511371,
         0.683982683982684,
         0.6568794810515535,
         0.5760761589403973,
         0.17468619346618652
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "tanh",
         0.5598360655737705,
         0.6908752327746741,
         0.6933286308286308,
         0.5793681033957203,
         0.5617381812511371,
         0.683982683982684,
         0.6568794810515535,
         0.5760761589403973,
         0.11877083778381348
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "tanh",
         0.5598360655737705,
         0.6908752327746741,
         0.6933286308286308,
         0.5793681033957203,
         0.5617381812511371,
         0.683982683982684,
         0.6568794810515535,
         0.5760761589403973,
         0.16115093231201172
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "tanh",
         0.778472116119175,
         0.7988826815642458,
         0.7791793033728518,
         0.7777921721636286,
         0.5523732458848832,
         0.6060606060606061,
         0.5550176056338028,
         0.5517384105960265,
         0.6389546394348145
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "tanh",
         0.778472116119175,
         0.7988826815642458,
         0.7791793033728518,
         0.7777921721636286,
         0.5523732458848832,
         0.6060606060606061,
         0.5550176056338028,
         0.5517384105960265,
         0.5658309459686279
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "tanh",
         0.778472116119175,
         0.7988826815642458,
         0.7791793033728518,
         0.7777921721636286,
         0.5523732458848832,
         0.6060606060606061,
         0.5550176056338028,
         0.5517384105960265,
         0.8675565719604492
        ],
        [
         [
          50,
          100,
          50
         ],
         "lbfgs",
         "tanh",
         0.778472116119175,
         0.7988826815642458,
         0.7791793033728518,
         0.7777921721636286,
         0.5523732458848832,
         0.6060606060606061,
         0.5550176056338028,
         0.5517384105960265,
         0.8620691299438477
        ],
        [
         [
          100
         ],
         "adam",
         "relu",
         0.6321077871660197,
         0.6927374301675978,
         0.6571936223818298,
         0.6274233371944157,
         0.551271796512558,
         0.6320346320346321,
         0.5691117235531058,
         0.5539735099337748,
         0.11571836471557617
        ],
        [
         [
          100
         ],
         "adam",
         "relu",
         0.6321077871660197,
         0.6927374301675978,
         0.6571936223818298,
         0.6274233371944157,
         0.551271796512558,
         0.6320346320346321,
         0.5691117235531058,
         0.5539735099337748,
         0.10072779655456543
        ],
        [
         [
          100
         ],
         "adam",
         "relu",
         0.6321077871660197,
         0.6927374301675978,
         0.6571936223818298,
         0.6274233371944157,
         0.551271796512558,
         0.6320346320346321,
         0.5691117235531058,
         0.5539735099337748,
         0.08798336982727051
        ],
        [
         [
          100
         ],
         "adam",
         "relu",
         0.6321077871660197,
         0.6927374301675978,
         0.6571936223818298,
         0.6274233371944157,
         0.551271796512558,
         0.6320346320346321,
         0.5691117235531058,
         0.5539735099337748,
         0.06615424156188965
        ],
        [
         [
          100
         ],
         "sgd",
         "tanh",
         0.6262879143921045,
         0.7132216014897579,
         0.7035048471290082,
         0.6247790038407608,
         0.5377238350183227,
         0.6666666666666666,
         0.6174462273813588,
         0.5569536423841059,
         0.0701742172241211
        ],
        [
         [
          100
         ],
         "sgd",
         "tanh",
         0.6262879143921045,
         0.7132216014897579,
         0.7035048471290082,
         0.6247790038407608,
         0.5377238350183227,
         0.6666666666666666,
         0.6174462273813588,
         0.5569536423841059,
         0.1341092586517334
        ],
        [
         [
          100
         ],
         "sgd",
         "tanh",
         0.6262879143921045,
         0.7132216014897579,
         0.7035048471290082,
         0.6247790038407608,
         0.5377238350183227,
         0.6666666666666666,
         0.6174462273813588,
         0.5569536423841059,
         0.14577150344848633
        ],
        [
         [
          100
         ],
         "sgd",
         "tanh",
         0.6262879143921045,
         0.7132216014897579,
         0.7035048471290082,
         0.6247790038407608,
         0.5377238350183227,
         0.6666666666666666,
         0.6174462273813588,
         0.5569536423841059,
         0.0716860294342041
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "relu",
         0.5518145398845292,
         0.6834264432029795,
         0.6710100064790152,
         0.5724105346582942,
         0.5375058281451415,
         0.683982683982684,
         0.6761904761904762,
         0.5643211920529801,
         0.10633683204650879
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "relu",
         0.5518145398845292,
         0.6834264432029795,
         0.6710100064790152,
         0.5724105346582942,
         0.5375058281451415,
         0.683982683982684,
         0.6761904761904762,
         0.5643211920529801,
         0.1101837158203125
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "relu",
         0.5518145398845292,
         0.6834264432029795,
         0.6710100064790152,
         0.5724105346582942,
         0.5375058281451415,
         0.683982683982684,
         0.6761904761904762,
         0.5643211920529801,
         0.13660240173339844
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "relu",
         0.5518145398845292,
         0.6834264432029795,
         0.6710100064790152,
         0.5724105346582942,
         0.5375058281451415,
         0.683982683982684,
         0.6761904761904762,
         0.5643211920529801,
         0.22317242622375488
        ],
        [
         [
          100
         ],
         "adam",
         "tanh",
         0.5908015975896017,
         0.6759776536312849,
         0.6346931071238231,
         0.5924449795769067,
         0.5021181716833891,
         0.6233766233766234,
         0.5351769295068264,
         0.5209023178807948,
         0.04518389701843262
        ],
        [
         [
          100
         ],
         "adam",
         "tanh",
         0.5908015975896017,
         0.6759776536312849,
         0.6346931071238231,
         0.5924449795769067,
         0.5021181716833891,
         0.6233766233766234,
         0.5351769295068264,
         0.5209023178807948,
         0.07428812980651855
        ],
        [
         [
          100
         ],
         "adam",
         "tanh",
         0.5908015975896017,
         0.6759776536312849,
         0.6346931071238231,
         0.5924449795769067,
         0.5021181716833891,
         0.6233766233766234,
         0.5351769295068264,
         0.5209023178807948,
         0.051788330078125
        ],
        [
         [
          100
         ],
         "adam",
         "tanh",
         0.5908015975896017,
         0.6759776536312849,
         0.6346931071238231,
         0.5924449795769067,
         0.5021181716833891,
         0.6233766233766234,
         0.5351769295068264,
         0.5209023178807948,
         0.055976152420043945
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "logistic",
         0.5014364292471009,
         0.6052141527001862,
         0.5214069994670457,
         0.5146924343107968,
         0.49921166732361055,
         0.6190476190476191,
         0.5289746386692119,
         0.517591059602649,
         0.1811237335205078
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "logistic",
         0.5014364292471009,
         0.6052141527001862,
         0.5214069994670457,
         0.5146924343107968,
         0.49921166732361055,
         0.6190476190476191,
         0.5289746386692119,
         0.517591059602649,
         0.13753128051757812
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "logistic",
         0.5014364292471009,
         0.6052141527001862,
         0.5214069994670457,
         0.5146924343107968,
         0.49921166732361055,
         0.6190476190476191,
         0.5289746386692119,
         0.517591059602649,
         0.2401723861694336
        ],
        [
         [
          50,
          100,
          50
         ],
         "adam",
         "logistic",
         0.5014364292471009,
         0.6052141527001862,
         0.5214069994670457,
         0.5146924343107968,
         0.49921166732361055,
         0.6190476190476191,
         0.5289746386692119,
         0.517591059602649,
         0.15000700950622559
        ],
        [
         [
          100
         ],
         "adam",
         "logistic",
         0.4957137286990714,
         0.6815642458100558,
         0.7713584841820336,
         0.5488934950923612,
         0.49514817728822447,
         0.6753246753246753,
         0.6833098094565984,
         0.5430049668874172,
         0.1176602840423584
        ],
        [
         [
          100
         ],
         "adam",
         "logistic",
         0.4957137286990714,
         0.6815642458100558,
         0.7713584841820336,
         0.5488934950923612,
         0.49514817728822447,
         0.6753246753246753,
         0.6833098094565984,
         0.5430049668874172,
         0.10259413719177246
        ],
        [
         [
          100
         ],
         "adam",
         "logistic",
         0.4957137286990714,
         0.6815642458100558,
         0.7713584841820336,
         0.5488934950923612,
         0.49514817728822447,
         0.6753246753246753,
         0.6833098094565984,
         0.5430049668874172,
         0.11374688148498535
        ],
        [
         [
          100
         ],
         "adam",
         "logistic",
         0.4957137286990714,
         0.6815642458100558,
         0.7713584841820336,
         0.5488934950923612,
         0.49514817728822447,
         0.6753246753246753,
         0.6833098094565984,
         0.5430049668874172,
         0.08202672004699707
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "tanh",
         0.4651873554193694,
         0.6480446927374302,
         0.5725384177575412,
         0.5194248003414009,
         0.4765810985228739,
         0.658008658008658,
         0.6,
         0.5268211920529802,
         0.14359140396118164
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "tanh",
         0.4651873554193694,
         0.6480446927374302,
         0.5725384177575412,
         0.5194248003414009,
         0.4765810985228739,
         0.658008658008658,
         0.6,
         0.5268211920529802,
         0.06280994415283203
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "tanh",
         0.4651873554193694,
         0.6480446927374302,
         0.5725384177575412,
         0.5194248003414009,
         0.4765810985228739,
         0.658008658008658,
         0.6,
         0.5268211920529802,
         0.05563211441040039
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "tanh",
         0.4651873554193694,
         0.6480446927374302,
         0.5725384177575412,
         0.5194248003414009,
         0.4765810985228739,
         0.658008658008658,
         0.6,
         0.5268211920529802,
         0.06773233413696289
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "relu",
         0.42954179798558617,
         0.5512104283054003,
         0.430738486692706,
         0.4535146009876242,
         0.4448736664795059,
         0.5367965367965368,
         0.4447973381730187,
         0.4546771523178808,
         0.07741045951843262
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "relu",
         0.42954179798558617,
         0.5512104283054003,
         0.430738486692706,
         0.4535146009876242,
         0.4448736664795059,
         0.5367965367965368,
         0.4447973381730187,
         0.4546771523178808,
         0.07506155967712402
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "relu",
         0.42954179798558617,
         0.5512104283054003,
         0.430738486692706,
         0.4535146009876242,
         0.4448736664795059,
         0.5367965367965368,
         0.4447973381730187,
         0.4546771523178808,
         0.0732572078704834
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "relu",
         0.42954179798558617,
         0.5512104283054003,
         0.430738486692706,
         0.4535146009876242,
         0.4448736664795059,
         0.5367965367965368,
         0.4447973381730187,
         0.4546771523178808,
         0.07546567916870117
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "tanh",
         0.4225130802534142,
         0.6405959031657356,
         0.5095001016053647,
         0.5014250441992318,
         0.3981609195402299,
         0.6320346320346321,
         0.3950892857142857,
         0.4863824503311258,
         0.22121381759643555
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "tanh",
         0.4225130802534142,
         0.6405959031657356,
         0.5095001016053647,
         0.5014250441992318,
         0.3981609195402299,
         0.6320346320346321,
         0.3950892857142857,
         0.4863824503311258,
         0.26922035217285156
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "tanh",
         0.4225130802534142,
         0.6405959031657356,
         0.5095001016053647,
         0.5014250441992318,
         0.3981609195402299,
         0.6320346320346321,
         0.3950892857142857,
         0.4863824503311258,
         0.22985124588012695
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "tanh",
         0.4225130802534142,
         0.6405959031657356,
         0.5095001016053647,
         0.5014250441992318,
         0.3981609195402299,
         0.6320346320346321,
         0.3950892857142857,
         0.4863824503311258,
         0.08966827392578125
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.14042186737060547
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.12438416481018066
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.159318208694458
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.20062494277954102
        ],
        [
         [
          50,
          50,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.1353003978729248
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.1016993522644043
        ],
        [
         [
          100
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.07140898704528809
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.1459331512451172
        ],
        [
         [
          100
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.04213213920593262
        ],
        [
         [
          100
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.04602384567260742
        ],
        [
         [
          100
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.08082866668701172
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.3765523433685303
        ],
        [
         [
          50,
          100,
          50
         ],
         "sgd",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.21715283393859863
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.08377265930175781
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.07906794548034668
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "logistic",
         0.39390519187358913,
         0.6499068901303539,
         0.3249534450651769,
         0.5,
         0.39528795811518325,
         0.6536796536796536,
         0.3268398268398268,
         0.5,
         0.09014225006103516
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "tanh",
         0.396781801179771,
         0.6443202979515829,
         0.4242481203007519,
         0.49692891544229717,
         0.39370078740157477,
         0.6493506493506493,
         0.32608695652173914,
         0.4966887417218543,
         0.06585979461669922
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "tanh",
         0.396781801179771,
         0.6443202979515829,
         0.4242481203007519,
         0.49692891544229717,
         0.39370078740157477,
         0.6493506493506493,
         0.32608695652173914,
         0.4966887417218543,
         0.08847403526306152
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "tanh",
         0.396781801179771,
         0.6443202979515829,
         0.4242481203007519,
         0.49692891544229717,
         0.39370078740157477,
         0.6493506493506493,
         0.32608695652173914,
         0.4966887417218543,
         0.10080909729003906
        ],
        [
         [
          50,
          50,
          50
         ],
         "adam",
         "tanh",
         0.396781801179771,
         0.6443202979515829,
         0.4242481203007519,
         0.49692891544229717,
         0.39370078740157477,
         0.6493506493506493,
         0.32608695652173914,
         0.4966887417218543,
         0.06584525108337402
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hidden_layer_sizes",
         "type": "{\"containsNull\":true,\"elementType\":\"long\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "solver",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "activation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "train_f1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_precision",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_recall",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "valid_f1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "valid_accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "valid_precision",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "valid_recall",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "training_time",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df.sort_values(by='valid_f1', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c25aa7-23fe-4057-a646-1a09267ce9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "neuralnetworktutorial2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}